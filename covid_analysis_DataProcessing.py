# -*- coding: utf-8 -*-
"""Covid_Analysis_Task-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ip5YUgO6SAWNjyx0suUmLH3bXZcq195o
"""

# Step 1: Import Libraries
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, desc, sum, avg

# Step 2: Initialize Spark Session
spark = SparkSession.builder \
    .appName("Big Data Analysis - COVID Dataset") \
    .getOrCreate()

# Step 3: Load Dataset
df = spark.read.csv("/content/Covid.csv", header=True, inferSchema=True)

# Step 4: Preview Schema & Data
df.printSchema()
df.show(5)

# Step 5: Clean & Filter
# Drop rows with null values in important columns
df_clean = df.dropna(subset=["Confirmed", "Deaths", "Recovered", "Active"])

# Step 6: Descriptive Statistics
df_clean.describe(["Confirmed", "Deaths", "Recovered", "Active"]).show()

# Step 7: Top 10 Countries by Confirmed Cases
top_confirmed = df_clean.select("Country/Region", "Confirmed") \
                        .orderBy(desc("Confirmed")) \
                        .limit(10)
top_confirmed.show()

# Step 8: Mortality Rate by Country
df_with_mortality = df_clean.withColumn("MortalityRate", (col("Deaths") / col("Confirmed")) * 100)
df_with_mortality.select("Country/Region", "Confirmed", "Deaths", "MortalityRate") \
                 .orderBy(desc("MortalityRate")) \
                 .show(10)

# Step 9: Average Recovery Rate
df_with_recovery = df_clean.withColumn("RecoveryRate", (col("Recovered") / col("Confirmed")) * 100)
df_with_recovery.select(avg("RecoveryRate").alias("Avg_Recovery_Rate")).show()

# Step 10: Total Global Stats
df_clean.select(
    sum("Confirmed").alias("Global Confirmed"),
    sum("Deaths").alias("Global Deaths"),
    sum("Recovered").alias("Global Recovered"),
    sum("Active").alias("Global Active")
).show()